import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32" # è¿›ä¸€æ­¥æ”¹å°ç¢ç‰‡é˜ˆå€¼

import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from accelerate import Accelerator, DistributedDataParallelKwargs
from accelerate.utils import set_seed
from torchvision.utils import save_image
import time
from datetime import datetime
# å¼•å…¥ 8-bit
try:
    import bitsandbytes as bnb
    HAS_BNB = True
except ImportError:
    HAS_BNB = False

import kagglehub
path = kagglehub.dataset_download("ifigotin/imagenetmini-1000")

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.models.sald_model import SALDModel
from src.data.sald_dataset import SyntheticRestorationDataset

CONFIG = {
    "data_path": path, 
    "save_path": "checkpoints/stage1_sald",
    "batch_size": 32,       
    "img_size": 256,       
    "grad_accum_steps": 1, 
    "epochs": 100,
    "lr": 1e-4,
    "seed": 42
}

# 1. ç”Ÿæˆå¸¦æ—¶é—´çš„å­è·¯å¾„ï¼ˆæ¨èæ ¼å¼ï¼šå¹´-æœˆ-æ—¥_æ—¶-åˆ†-ç§’ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦ï¼‰
# æ—¶é—´æ ¼å¼å¯è‡ªå®šä¹‰ï¼Œæ¯”å¦‚ "%Y%m%d_%H%M%S" æ˜¯çº¯æ•°å­—æ ¼å¼
time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

# 2. æ‹¼æ¥æ–°çš„save_pathï¼šåŸè·¯å¾„ + æ—¶é—´å­è·¯å¾„
original_save_path = CONFIG["save_path"]
new_save_path = os.path.join(original_save_path, time_str)

# 3. ä½¿ç”¨CONFIG.update()æ›´æ–°é…ç½®ï¼ˆæ ¸å¿ƒæ“ä½œï¼‰
CONFIG.update({"save_path": new_save_path})

@torch.no_grad()
def save_preview(model, dataloader, epoch, step, save_path, accelerator):
    """
    ä¿å­˜è®­ç»ƒé¢„è§ˆå›¾ï¼šå±•ç¤º IRã€Visã€GT ä»¥åŠæ¨¡å‹å½“å‰çš„é¢„æµ‹ç»“æœ (x0)ã€‚
    æ­¤å‡½æ•°ç”¨äºç›´è§‚ç›‘æ§ Stage 1 è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å¯¹æç«¯é€€åŒ–å›¾åƒçš„ä¿®å¤èƒ½åŠ›ã€‚
    æ¶‰åŠåˆ°ä¸­æ–‡çš„åœ°æ–¹å·²è½¬ä¸ºç®€ä½“ä¸­æ–‡ã€‚
    """
    # 1. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œé˜²æ­¢ Dropout ç­‰å±‚å¹²æ‰°
    model.eval()
    unwrapped_model = accelerator.unwrap_model(model)
    
    # 2. è·å–ä¸€ç»„é¢„è§ˆæ•°æ®
    try:
        # è·å–ä¸€ä¸ª batch çš„æ•°æ®ç”¨äºé¢„è§ˆ
        batch = next(iter(dataloader))
        ir, vis, gt = batch
    except Exception as e:
        if accelerator.is_main_process:
            print(f"âš ï¸ é¢„è§ˆå›¾ä¿å­˜å¤±è´¥ï¼Œæ— æ³•è·å–æ•°æ®: {e}")
        model.train()
        return

    # ä»…å–å‰ 4 å¼ æ ·æœ¬ï¼ˆè‹¥ä¸è¶³ 4 å¼ åˆ™å–å…¨éƒ¨ï¼‰ï¼Œé˜²æ­¢é¢„è§ˆå›¾è¿‡å¤§å¯¼è‡´ä¿å­˜ç¼“æ…¢
    num_samples = min(ir.shape[0], 4)
    ir = ir[:num_samples].to(accelerator.device)
    vis = vis[:num_samples].to(accelerator.device)
    gt = gt[:num_samples].to(accelerator.device)

    # 3. æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ï¼šé¢„æµ‹åŸå›¾ x0
    # æ­¥éª¤ A: å°† GT ç¼–ç ä¸º Latent ç©ºé—´ç‰¹å¾ (VAE æœŸæœ›èŒƒå›´ä¸º [-1, 1])
    latents = unwrapped_model.encode_latents(gt) # å½¢çŠ¶ [B, 4, H/8, W/8]
    
    # æ­¥éª¤ B: é‡‡æ ·ä¸€ä¸ªå›ºå®šçš„ä¸­é—´æ—¶é—´æ­¥ (ä¾‹å¦‚ 500)ï¼Œè§‚å¯Ÿä¸­ç­‰å¼ºåº¦å™ªå£°ä¸‹çš„è¿˜åŸæ•ˆæœ
    # ç›¸æ¯”éšæœºé‡‡æ ·ï¼Œå›ºå®šæ—¶é—´æ­¥æ›´èƒ½ä½“ç°æ¨¡å‹éš Epoch å¢é•¿è€Œäº§ç”Ÿçš„æ€§èƒ½æå‡
    timesteps = torch.tensor([500] * num_samples, device=latents.device).long()
    noise = torch.randn_like(latents)
    noisy_latents = unwrapped_model.scheduler.add_noise(latents, noise, timesteps)

    # æ­¥éª¤ C: æå–ç»“æ„ç‰¹å¾å¹¶è¿›è¡Œæ—¶å˜ç²¾ç‚¼ (å¤ç”¨ SALD æ ¸å¿ƒé€»è¾‘)
    # æ ¹æ®ä½ æœ€æ–°çš„ä¿®æ”¹ï¼Œl_sgb ç°åœ¨ç›´æ¥è¿”å›æœ€ç»ˆçš„ç‰¹å¾ Tensor
    sgb_feats = unwrapped_model.l_sgb(ir.float(), vis.float()) 
    raw_cond = sgb_feats 
    
    b, c, h, w = raw_cond.shape
    raw_cond_flat = raw_cond.view(b, c, -1).permute(0, 2, 1) # [B, Seq, 256]

    # æ˜ å°„å¹¶ç²¾ç‚¼ç‰¹å¾ (768 ç»´)
    aligned_cond = unwrapped_model.adapter(raw_cond_flat)
    
    # ç”Ÿæˆé¢„è§ˆå¯¹åº”çš„æ—¶é—´æ­¥åµŒå…¥
    t_val = timesteps.float().view(num_samples, 1) / 1000.0
    t_emb = unwrapped_model.time_proj(t_val)
    
    refined_cond = unwrapped_model.tc_refinement(
        x=aligned_cond, 
        context=aligned_cond,
        time_emb=t_emb
    )

    # 4. è°ƒç”¨ U-Net é¢„æµ‹å™ªå£° (ä½¿ç”¨æ··åˆç²¾åº¦ä»¥åŒ¹é…è®­ç»ƒç¯å¢ƒ)
    with torch.amp.autocast('cuda', enabled=(accelerator.mixed_precision != "no")):
        noise_pred = unwrapped_model.unet(
            noisy_latents.to(unwrapped_model.unet.dtype), 
            timesteps, 
            encoder_hidden_states=refined_cond.to(unwrapped_model.unet.dtype)
        ).sample

    # 5. æ ¹æ®æ‰©æ•£å»å™ªå…¬å¼åæ¨åŸå§‹æ ·æœ¬ x0 (Original Sample)
    # å…¬å¼: x0 = (x_t - sqrt(1 - alpha_bar_t) * epsilon) / sqrt(alpha_bar_t)
    alpha_prod_t = unwrapped_model.scheduler.alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(latents.device)
    
    pred_latents = (noisy_latents - (1 - alpha_prod_t) ** 0.5 * noise_pred) / (alpha_prod_t ** 0.5)
    
    # 1. ä¿®å¤ç²¾åº¦ä¸åŒ¹é…é”™è¯¯
    # ç¡®ä¿ pred_latents è½¬æ¢ä¸º VAE çš„ç²¾åº¦ (é€šå¸¸æ˜¯ float16)
    pred_latents_input = (pred_latents / 0.18215).to(unwrapped_model.vae.dtype)

    # 2. VAE è§£ç é¢„æµ‹å‡ºçš„ Latent åˆ°åƒç´ ç©ºé—´
    pred_imgs = unwrapped_model.vae.decode(pred_latents_input).sample # è¾“å‡ºèŒƒå›´çº¦ä¸º [-1, 1]
    
    

    # 7. æ‹¼æ¥å¯¹æ¯”å›¾å¹¶ä¿å­˜
    # å°†å›¾åƒç»Ÿä¸€è½¬æ¢å› [0, 1] èŒƒå›´ä»¥ä¾¿å±•ç¤º
    gt_display = (gt + 1.0) / 2.0
    pred_display = (pred_imgs + 1.0) / 2.0
    
    # å°†å•é€šé“çš„ IR å’Œ Vis æ¡ä»¶å›¾æ‰©å±•ä¸º 3 é€šé“ï¼Œæ–¹ä¾¿æ¨ªå‘æ‹¼æ¥
    ir_display = ir.repeat(1, 3, 1, 1)
    vis_display = vis.repeat(1, 3, 1, 1)

    # æ¯è¡Œå›¾ç‰‡æ’å¸ƒï¼š[é€€åŒ–çº¢å¤–, é€€åŒ–å¯è§å…‰, åŸå§‹çœŸå€¼GT, æ¨¡å‹å½“å‰é¢„æµ‹ç»“æœ]
    # dim=3 æ˜¯åœ¨å®½åº¦æ–¹å‘æ‹¼æ¥
    comparison = torch.cat([ir_display, vis_display, gt_display, pred_display], dim=3)
    
    # æ„é€ æ–‡ä»¶å
    save_name = f"preview_e{epoch+1}_s{step:04d}.png"
    save_full_path = os.path.join(save_path, save_name)
    
    # nrow=1 è¡¨ç¤ºæ¯ä¸€è¡Œæ˜¾ç¤ºä¸€ä¸ªæ ·æœ¬åŠå…¶å¯¹åº”çš„å››é¡¹å¯¹æ¯”
    save_image(comparison.float(), save_full_path, nrow=1, normalize=False)
    
    if accelerator.is_main_process:
        print(f"ğŸ“¸ é¢„è§ˆå›¾å·²æˆåŠŸä¿å­˜: {save_full_path}")
    
    # æ¢å¤æ¨¡å‹è‡³è®­ç»ƒæ¨¡å¼
    model.train()

def train():
    # === [å…³é”®ä¿®æ”¹] ç¦ç”¨ cuDNN Benchmark ä»¥èŠ‚çœæ˜¾å­˜ ===
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    # ===============================================
    # é’ˆå¯¹ L-SGB ä¸­æœªå‚ä¸ loss è®¡ç®—çš„å¤šå°ºåº¦ç‰¹å¾å±‚ 
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    accelerator = Accelerator(
        mixed_precision="fp16",
        gradient_accumulation_steps=CONFIG['grad_accum_steps'],
        kwargs_handlers=[ddp_kwargs] # æ³¨å…¥é…ç½®
    )
    set_seed(CONFIG['seed'])

    preview_save_path = os.path.join(CONFIG['save_path'], "previews")

    if accelerator.is_main_process:
        os.makedirs(CONFIG['save_path'], exist_ok=True)
        os.makedirs(preview_save_path, exist_ok=True)
        print(f"ğŸš€ Launching Distributed Training on {accelerator.num_processes} GPUs!")

    # æ•°æ®
    dataset = SyntheticRestorationDataset(CONFIG['data_path'], size=CONFIG['img_size'])
    dataloader = DataLoader(
        dataset, 
        batch_size=CONFIG['batch_size'], 
        shuffle=True, 
        num_workers=14, 
        pin_memory=True,
        drop_last=True
    )
    
    # æ¨¡å‹ (ç°åœ¨é»˜è®¤åŠ è½½ FP16)
    model = SALDModel() 
    
    # æ˜¾å­˜ä¼˜åŒ–
    model.vae.enable_slicing()
    model.vae.enable_tiling()
    
    # å°è¯•å¼€å¯ xFormers (å¦‚æœä½ è£…äº†çš„è¯ï¼Œè¿™æ˜¯çœæ˜¾å­˜ç¥å™¨)
    try:
        model.unet.enable_xformers_memory_efficient_attention()
    except:
        pass # æ²¡è£…å°±ç®—äº†

    if hasattr(model.unet, "enable_gradient_checkpointing"):
        model.unet.enable_gradient_checkpointing()

    # ä¼˜åŒ–å™¨
    params_to_optimize = [p for p in model.parameters() if p.requires_grad]
    if HAS_BNB:
        # 8-bit AdamW
        optimizer = bnb.optim.AdamW8bit(params_to_optimize, lr=CONFIG['lr'])
    else:
        optimizer = torch.optim.AdamW(params_to_optimize, lr=CONFIG['lr'])
    
    model, optimizer, dataloader = accelerator.prepare(
        model, optimizer, dataloader
    )

    if accelerator.is_main_process:
        print("Start Training...")
        
    model.train()
    if hasattr(model, "module"):
        model.module.vae.eval()
    else:
        model.vae.eval()


    # éå†æ¯ä¸ªepoch
    for epoch in range(CONFIG['epochs']):
        # -------- æ–°å¢ï¼šåˆå§‹åŒ–epochçº§åˆ«çš„lossç»Ÿè®¡å˜é‡ --------
        epoch_loss_sum = 0.0  # ç´¯åŠ å½“å‰epochçš„æ‰€æœ‰loss
        epoch_step_count = 0   # ç»Ÿè®¡å½“å‰epochçš„stepæ•°
        
        if accelerator.is_main_process:
            loop = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        else:
            loop = dataloader 
            
        for step, (ir, vis, gt) in enumerate(loop):
            with accelerator.accumulate(model): 
                loss = model(ir, vis, gt)
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
            
            # -------- æ–°å¢ï¼šç´¯åŠ losså’Œstepæ•° --------
            # åªåœ¨ä¸»è¿›ç¨‹ç»Ÿè®¡ï¼ˆé¿å…å¤šè¿›ç¨‹é‡å¤ç´¯åŠ ï¼‰
            if accelerator.is_main_process:
                epoch_loss_sum += loss.item()  # ç´¯åŠ å½“å‰stepçš„losså€¼
                epoch_step_count += 1  # stepæ•°+1
            
            # --- åŸæœ‰ç›‘æ§ä»£ç  ---
            if step % 500 == 0 and accelerator.is_main_process:
                save_preview(model, dataloader, epoch, step, preview_save_path, accelerator)
            # ------------------
            
            if accelerator.is_main_process:
                loop.set_postfix(loss=loss.item())

        # -------- æ–°å¢ï¼šepochç»“æŸæ—¶è®¡ç®—å¹¶æ‰“å°å¹³å‡loss --------
        if accelerator.is_main_process:
            # è®¡ç®—å¹³å‡lossï¼ˆé¿å…é™¤ä»¥0ï¼‰
            if epoch_step_count > 0:
                epoch_avg_loss = epoch_loss_sum / epoch_step_count
            else:
                epoch_avg_loss = 0.0
            
            # æ ¼å¼åŒ–æ‰“å°ï¼Œä¿ç•™4ä½å°æ•°æ›´æ˜“è¯»
            print(f"\nğŸ“Š Epoch {epoch+1} finished | Average Loss: {epoch_avg_loss:.4f}")
            
            # åŸæœ‰ä¿å­˜checkpointä»£ç 
            unwrapped_model = accelerator.unwrap_model(model)
            torch.save(unwrapped_model.state_dict(), os.path.join(CONFIG['save_path'], "sald_stage1_latest.pth"))
            print(f"âœ… Saved checkpoint for Epoch {epoch+1}\n")

if __name__ == "__main__":
    train()