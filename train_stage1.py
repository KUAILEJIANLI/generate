import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# é™ä½ç¢ç‰‡åŒ–é£é™©
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.models.sald_model import SALDModel
from src.data.sald_dataset import SyntheticRestorationDataset

CONFIG = {
    "device": "cuda",
    "data_path": "/home/tf/dataset/mini_imageNet", 
    "save_path": "checkpoints/stage1_sald",
    
    # === [ç»å¢ƒé…ç½®] ===
    "img_size": 256,       # <--- æ ¸å¿ƒä¿®æ”¹: ä» 512 é™åˆ° 256 (æ˜¾å­˜å ç”¨ -70%)
    "batch_size": 4,       # 4å¼ å¡ï¼Œæ¯å¼ å¡è·‘1ä¸ª (Total 4)
    "grad_accum_steps": 8, # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆ Batch = 32
    "epochs": 10,
    "lr": 1e-5
}

def train():
    os.makedirs(CONFIG['save_path'], exist_ok=True)
    
    # 1. æ£€æµ‹ GPU
    gpu_count = torch.cuda.device_count()
    print(f"âš¡ Detected {gpu_count} GPUs. Average VRAM per card: ~10GB (Estimated)")
    
    # 2. æ•°æ® (æ³¨æ„è¿™é‡Œçš„ size)
    print(f"Initializing Dataset (Size={CONFIG['img_size']})...")
    dataset = SyntheticRestorationDataset(CONFIG['data_path'], size=CONFIG['img_size'])
    dataloader = DataLoader(
        dataset, 
        batch_size=CONFIG['batch_size'], 
        shuffle=True, 
        num_workers=4, # é™ä½ worker æ•°é‡ä»¥å‡å°‘ CPU å†…å­˜å¼€é”€
        pin_memory=True,
        drop_last=True
    )
    
    # 3. æ¨¡å‹
    print("Initializing SALD Model...")
    model = SALDModel(device="cuda:0").to("cuda:0")
    
    # =======================================================
    # ğŸ›¡ï¸ æ˜¾å­˜ä¿å«æˆ˜ (MAXIMUM SAVING)
    # =======================================================
    
    # [1] æ¢¯åº¦æ£€æŸ¥ç‚¹ (å¿…å¼€)
    model.unet.enable_gradient_checkpointing()
    
    # [2] VAE ä¼˜åŒ– (å¿…å¼€)
    model.vae.enable_slicing()
    model.vae.enable_tiling()
    
    # [3] Attention åˆ‡ç‰‡ (å¿…å¼€ - æ›¿ä»£ xformers)
    # å¦‚æœæ²¡æœ‰ xformersï¼Œè¿™ä¸ªå‡½æ•°èƒ½æ•‘å‘½ã€‚å®ƒæŠŠè®¡ç®—æ‹†å¾—éå¸¸ç¢ã€‚
    if hasattr(model.unet, "set_attention_slice"):
        model.unet.set_attention_slice("auto")
        print("âœ… Attention Slicing enabled (auto)!")
    
    # å°è¯•å¼€å¯ xformers (å¦‚æœæœ‰çš„è¯æ›´å¥½)
    try:
        model.unet.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers also enabled!")
    except:
        pass
        
    # =======================================================
    
    # 4. ä¼˜åŒ–å™¨
    trainable_params = list(model.l_sgb.parameters()) + \
                       list(model.adapter.parameters()) + \
                       list(model.unet.parameters())
    optimizer = torch.optim.AdamW(trainable_params, lr=CONFIG['lr'])
    
    # 5. å¤šå¡å¹¶è¡Œ
    if gpu_count > 1:
        print(f"ğŸš€ Activating DataParallel on IDs: {list(range(gpu_count))}")
        model = nn.DataParallel(model)
    
    scaler = GradScaler()

    # 6. è®­ç»ƒ
    print("Start Training...")
    model.train()
    
    # DataParallel ä¸‹è®¿é—® module å±æ€§
    if isinstance(model, nn.DataParallel):
        model.module.vae.eval()
    else:
        model.vae.eval()
    
    for epoch in range(CONFIG['epochs']):
        loop = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        loss_sum = 0
        
        for step, (ir, vis, gt) in enumerate(loop):
            ir = ir.cuda()
            vis = vis.cuda()
            gt = gt.cuda()
            
            # æ¸…ç†ç¼“å­˜ (ç¨å¾®ç‰ºç‰²é€Ÿåº¦ï¼Œé˜²æ­¢ç¢ç‰‡åŒ– OOM)
            # torch.cuda.empty_cache() 
            
            with autocast():
                loss = model(ir, vis, gt)
                
                if gpu_count > 1:
                    loss = loss.mean()
                
                loss = loss / CONFIG['grad_accum_steps']
            
            scaler.scale(loss).backward()
            
            if (step + 1) % CONFIG['grad_accum_steps'] == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            current_loss = loss.item() * CONFIG['grad_accum_steps']
            loss_sum += current_loss
            loop.set_postfix(loss=current_loss)
            
        print(f"Epoch {epoch+1} Avg Loss: {loss_sum/len(dataloader):.4f}")
        
        if gpu_count > 1:
            save_dict = model.module.state_dict()
        else:
            save_dict = model.state_dict()
        torch.save(save_dict, os.path.join(CONFIG['save_path'], "sald_stage1_latest.pth"))

if __name__ == "__main__":
    train()